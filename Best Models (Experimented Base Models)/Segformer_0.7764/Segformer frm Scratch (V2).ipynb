{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f24653e-25ce-48e0-a5e9-1510261ef188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93a7597-a7b7-4653-a4d0-2abfe68c4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from torchvision.ops import StochasticDepth\n",
    "from typing import List\n",
    "from typing import Iterable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "218b6dc8-4391-4a7a-9a8c-212ca7bfde7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMaskDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, filenames, transform_image=None, transform_mask=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_filenames = filenames\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_mask = transform_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_name = img_name  # Assuming filenames match\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        # Load image and mask\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform_image:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_mask:\n",
    "            mask = self.transform_mask(mask)\n",
    "\n",
    "        return {'pixel_values': image, 'labels': mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049ced1c-92b5-4b1a-a1a2-3270c18a5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize using ImageNet mean and std if desired\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_mask = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=Image.NEAREST),\n",
    "    transforms.PILToTensor(),  # Converts to tensor without normalization\n",
    "    transforms.Lambda(lambda x: x.squeeze().long())  # Remove channel dimension and convert to long\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7c0890-108c-4acd-ba2b-cd5b5320ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the paths to your image and mask directories\n",
    "image_dir = r'C:/Users/yifen/Desktop/Desktop/Yi Fen Folder/MITB/Deep Learning for Visual Recognition/Project/Project Dataset/train_images'\n",
    "mask_dir = r'C:/Users/yifen/Desktop/Desktop/Yi Fen Folder/MITB/Deep Learning for Visual Recognition/Project/Project Dataset/train_masks'\n",
    "all_filenames = sorted(os.listdir(image_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c1f47c-0ece-4149-be65-6ebdac038786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, \"b c h w -> b h w c\")\n",
    "        x = super().forward(x)\n",
    "        x = rearrange(x, \"b h w c -> b c h w\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class OverlapPatchMerging(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, patch_size: int, overlap_size: int\n",
    "    ):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=patch_size,\n",
    "                stride=overlap_size,\n",
    "                padding=patch_size // 2,\n",
    "                bias=False\n",
    "            ),\n",
    "            LayerNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "class EfficientMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, channels: int, reduction_ratio: int = 1, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.reducer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels, channels, kernel_size=reduction_ratio, stride=reduction_ratio\n",
    "            ),\n",
    "            LayerNorm2d(channels),\n",
    "        )\n",
    "        self.att = nn.MultiheadAttention(\n",
    "            channels, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, h, w = x.shape\n",
    "        reduced_x = self.reducer(x)\n",
    "        # attention needs tensor of shape (batch, sequence_length, channels)\n",
    "        reduced_x = rearrange(reduced_x, \"b c h w -> b (h w) c\")\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        out = self.att(x, reduced_x, reduced_x)[0]\n",
    "        # reshape it back to (batch, channels, height, width)\n",
    "        out = rearrange(out, \"b (h w) c -> b c h w\", h=h, w=w)\n",
    "        return out\n",
    "    \n",
    "class MixMLP(nn.Sequential):\n",
    "    def __init__(self, channels: int, expansion: int = 4):\n",
    "        super().__init__(\n",
    "            # dense layer\n",
    "            nn.Conv2d(channels, channels, kernel_size=1),\n",
    "            # depth wise conv\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                channels * expansion,\n",
    "                kernel_size=3,\n",
    "                groups=channels,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            # dense layer\n",
    "            nn.Conv2d(channels * expansion, channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    \"\"\"Just an util layer\"\"\"\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        out = self.fn(x, **kwargs)\n",
    "        x = x + out\n",
    "        return x\n",
    "\n",
    "class SegFormerEncoderBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction_ratio: int = 1,\n",
    "        num_heads: int = 8,\n",
    "        mlp_expansion: int = 4,\n",
    "        drop_path_prob: float = .0\n",
    "    ):\n",
    "        super().__init__(\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    LayerNorm2d(channels),\n",
    "                    EfficientMultiHeadAttention(channels, reduction_ratio, num_heads),\n",
    "                )\n",
    "            ),\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    LayerNorm2d(channels),\n",
    "                    MixMLP(channels, expansion=mlp_expansion),\n",
    "                    StochasticDepth(p=drop_path_prob, mode=\"batch\")\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "class SegFormerEncoderStage(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        patch_size: int,\n",
    "        overlap_size: int,\n",
    "        drop_probs: List[int],\n",
    "        depth: int = 2,\n",
    "        reduction_ratio: int = 1,\n",
    "        num_heads: int = 8,\n",
    "        mlp_expansion: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.overlap_patch_merge = OverlapPatchMerging(\n",
    "            in_channels, out_channels, patch_size, overlap_size,\n",
    "        )\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                SegFormerEncoderBlock(\n",
    "                    out_channels, reduction_ratio, num_heads, mlp_expansion, drop_probs[i]\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = LayerNorm2d(out_channels)\n",
    "\n",
    "\n",
    "def chunks(data: Iterable, sizes: List[int]):\n",
    "    \"\"\"\n",
    "    Given an iterable, returns slices using sizes as indices\n",
    "    \"\"\"\n",
    "    curr = 0\n",
    "    for size in sizes:\n",
    "        chunk = data[curr: curr + size]\n",
    "        curr += size\n",
    "        yield chunk\n",
    "        \n",
    "class SegFormerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        widths: List[int],\n",
    "        depths: List[int],\n",
    "        all_num_heads: List[int],\n",
    "        patch_sizes: List[int],\n",
    "        overlap_sizes: List[int],\n",
    "        reduction_ratios: List[int],\n",
    "        mlp_expansions: List[int],\n",
    "        drop_prob: float = .0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # create drop paths probabilities (one for each stage's block)\n",
    "        drop_probs =  [x.item() for x in torch.linspace(0, drop_prob, sum(depths))]\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                SegFormerEncoderStage(*args)\n",
    "                for args in zip(\n",
    "                    [in_channels, *widths],\n",
    "                    widths,\n",
    "                    patch_sizes,\n",
    "                    overlap_sizes,\n",
    "                    chunks(drop_probs, sizes=depths),\n",
    "                    depths,\n",
    "                    reduction_ratios,\n",
    "                    all_num_heads,\n",
    "                    mlp_expansions\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "            features.append(x)\n",
    "        return features\n",
    "\n",
    "class SegFormerDecoderBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, scale_factor: int = 2):\n",
    "        super().__init__(\n",
    "            nn.UpsamplingBilinear2d(scale_factor=scale_factor),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "class SegFormerDecoder(nn.Module):\n",
    "    def __init__(self, out_channels: int, widths: List[int], scale_factors: List[int]):\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                SegFormerDecoderBlock(in_channels, out_channels, scale_factor)\n",
    "                for in_channels, scale_factor in zip(widths, scale_factors)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        new_features = []\n",
    "        for feature, stage in zip(features,self.stages):\n",
    "            x = stage(feature)\n",
    "            new_features.append(x)\n",
    "        return new_features\n",
    "\n",
    "class SegFormerSegmentationHead(nn.Module):\n",
    "    def __init__(self, channels: int, num_classes: int, num_features: int = 4):\n",
    "        super().__init__()\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv2d(channels * num_features, channels, kernel_size=1, bias=False),\n",
    "            nn.ReLU(),  # Applies ReLU activation to introduce non-linearity\n",
    "            nn.BatchNorm2d(channels)  # BatchNorm for better stability and convergence\n",
    "        )\n",
    "        self.predict = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
    "        # Add an upsampling layer to match the resolution of 224x224\n",
    "        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = torch.cat(features, dim=1)\n",
    "        x = self.fuse(x)\n",
    "        x = self.predict(x)\n",
    "        x = self.upsample(x)  # Upsample to 224x224\n",
    "        return x\n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        widths: List[int],\n",
    "        depths: List[int],\n",
    "        all_num_heads: List[int],\n",
    "        patch_sizes: List[int],\n",
    "        overlap_sizes: List[int],\n",
    "        reduction_ratios: List[int],\n",
    "        mlp_expansions: List[int],\n",
    "        decoder_channels: int,\n",
    "        scale_factors: List[int],\n",
    "        num_classes: int,\n",
    "        drop_prob: float = 0.0,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = SegFormerEncoder(\n",
    "            in_channels,\n",
    "            widths,\n",
    "            depths,\n",
    "            all_num_heads,\n",
    "            patch_sizes,\n",
    "            overlap_sizes,\n",
    "            reduction_ratios,\n",
    "            mlp_expansions,\n",
    "            drop_prob,\n",
    "        )\n",
    "        self.decoder = SegFormerDecoder(decoder_channels, widths[::-1], scale_factors)\n",
    "        self.head = SegFormerSegmentationHead(\n",
    "            decoder_channels, num_classes, num_features=len(widths)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        features = self.decoder(features[::-1])\n",
    "        segmentation = self.head(features)\n",
    "        return segmentation\n",
    "\n",
    "segformer = SegFormer(\n",
    "    in_channels=3,\n",
    "    widths=[64, 128, 256, 512],\n",
    "    depths=[3, 4, 6, 3],\n",
    "    all_num_heads=[1, 2, 4, 8],\n",
    "    patch_sizes=[7, 3, 3, 3],\n",
    "    overlap_sizes=[4, 2, 2, 2],\n",
    "    reduction_ratios=[8, 4, 2, 1],\n",
    "    mlp_expansions=[4, 4, 4, 4],\n",
    "    decoder_channels=256,\n",
    "    scale_factors=[8, 4, 2, 1],\n",
    "    num_classes=9,\n",
    ")\n",
    "\n",
    "# segmentation = segformer(torch.randn((1, 3, 224, 224)))\n",
    "# segmentation.shape # torch.Size([1, 100, 56, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c001cbaf-d775-471d-9000-d91765318b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 1467\n",
      "Validation images: 163\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "total_images = 1631\n",
    "\n",
    "# Calculate the number of images for each set\n",
    "train_size = int(0.90 * total_images)\n",
    "val_size = int(0.10 * total_images)\n",
    "# test_size = total_images - train_size - val_size  # Adjust for any rounding errors\n",
    "\n",
    "print(f\"Training images: {train_size}\")\n",
    "print(f\"Validation images: {val_size}\")\n",
    "# print(f\"Test images: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62251eeb-fa9e-4e19-a289-a1b11fbc8535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1631\n",
      "Training images: 1468\n",
      "Validation images: 163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split into training and temp (validation + test)\n",
    "train_filenames, val_filenames = train_test_split(all_filenames,test_size=(val_size),random_state=42)\n",
    "\n",
    "# Now split temp into validation and test\n",
    "# val_filenames, test_filenames = train_test_split(temp_filenames,test_size=test_size,random_state=42)\n",
    "\n",
    "print(f\"Total images: {len(all_filenames)}\")\n",
    "print(f\"Training images: {len(train_filenames)}\")\n",
    "print(f\"Validation images: {len(val_filenames)}\")\n",
    "# print(f\"Test images: {len(test_filenames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96831e97-dded-4d83-a027-d9dda3e31ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset and loader\n",
    "train_dataset = ImageMaskDataset(\n",
    "    image_dir=image_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    filenames=train_filenames,\n",
    "    transform_image=transform_image,\n",
    "    transform_mask=transform_mask\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "# Validation dataset and loader\n",
    "val_dataset = ImageMaskDataset(\n",
    "    image_dir=image_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    filenames=val_filenames,\n",
    "    transform_image=transform_image,\n",
    "    transform_mask=transform_mask\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "# Test dataset and loader\n",
    "# test_dataset = ImageMaskDataset(\n",
    "#     image_dir=image_dir,\n",
    "#     mask_dir=mask_dir,\n",
    "#     filenames=test_filenames,\n",
    "#     transform_image=transform_image,\n",
    "#     transform_mask=transform_mask\n",
    "# )\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "413d4554-731c-425f-9607-be43cb8c1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(preds, labels, num_classes):\n",
    "    \"\"\"\n",
    "    Computes IoU for each class between predicted labels and ground truth labels.\n",
    "\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted labels, shape (N, H, W)\n",
    "        labels (torch.Tensor): Ground truth labels, shape (N, H, W)\n",
    "        num_classes (int): Number of classes\n",
    "\n",
    "    Returns:\n",
    "        list: IoU for each class\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    preds = preds.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = preds == cls\n",
    "        target_inds = labels == cls\n",
    "        intersection = torch.sum((pred_inds & target_inds).float()).item()\n",
    "        union = torch.sum((pred_inds | target_inds).float()).item()\n",
    "        if union == 0:\n",
    "            iou = float('nan')  # If there is no ground truth, set IoU to NaN\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        ious.append(iou)\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82611a3-ca30-4d1e-833a-030a4798147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "import typing\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from torch import Tensor\n",
    "\n",
    "def soft_dice_score(\n",
    "    output: torch.Tensor, target: torch.Tensor, smooth: float = 0.0, eps: float = 1e-7, dims=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    :param output:\n",
    "    :param target:\n",
    "    :param smooth:\n",
    "    :param eps:\n",
    "    :return:\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, NC, *)` where :math:`*` means any number\n",
    "            of additional dimensions\n",
    "        - Target: :math:`(N, NC, *)`, same shape as the input\n",
    "        - Output: scalar.\n",
    "\n",
    "    \"\"\"\n",
    "    assert output.size() == target.size()\n",
    "    if dims is not None:\n",
    "        intersection = torch.sum(output * target, dim=dims)\n",
    "        cardinality = torch.sum(output + target, dim=dims)\n",
    "    else:\n",
    "        intersection = torch.sum(output * target)\n",
    "        cardinality = torch.sum(output + target)\n",
    "    dice_score = (2.0 * intersection + smooth) / (cardinality + smooth).clamp_min(eps)\n",
    "    return dice_score\n",
    "\n",
    "\n",
    "__all__ = [\"DiceLoss\"]\n",
    "\n",
    "BINARY_MODE = \"binary\"\n",
    "MULTICLASS_MODE = \"multiclass\"\n",
    "MULTILABEL_MODE = \"multilabel\"\n",
    "\n",
    "class DiceLoss(_Loss):\n",
    "    \"\"\"\n",
    "    Implementation of Dice loss for image segmentation task.\n",
    "    It supports binary, multiclass and multilabel cases\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        classes: List[int] = None,\n",
    "        log_loss=False,\n",
    "        from_logits=True,\n",
    "        smooth: float = 0.0,\n",
    "        ignore_index=None,\n",
    "        eps=1e-7,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param mode: Metric mode {'binary', 'multiclass', 'multilabel'}\n",
    "        :param classes: Optional list of classes that contribute in loss computation;\n",
    "        By default, all channels are included.\n",
    "        :param log_loss: If True, loss computed as `-log(jaccard)`; otherwise `1 - jaccard`\n",
    "        :param from_logits: If True assumes input is raw logits\n",
    "        :param smooth:\n",
    "        :param ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n",
    "        :param eps: Small epsilon for numerical stability\n",
    "        \"\"\"\n",
    "        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.mode = mode\n",
    "        if classes is not None:\n",
    "            assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n",
    "            classes = to_tensor(classes, dtype=torch.long)\n",
    "\n",
    "        self.classes = classes\n",
    "        self.from_logits = from_logits\n",
    "        self.smooth = smooth\n",
    "        self.eps = eps\n",
    "        self.ignore_index = ignore_index\n",
    "        self.log_loss = log_loss\n",
    "\n",
    "    def forward(self, y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        :param y_pred: NxCxHxW\n",
    "        :param y_true: NxHxW\n",
    "        :return: scalar\n",
    "        \"\"\"\n",
    "        assert y_true.size(0) == y_pred.size(0)\n",
    "\n",
    "        if self.from_logits:\n",
    "            # Apply activations to get [0..1] class probabilities\n",
    "            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n",
    "            # extreme values 0 and 1\n",
    "            if self.mode == MULTICLASS_MODE:\n",
    "                y_pred = y_pred.log_softmax(dim=1).exp()\n",
    "            else:\n",
    "                y_pred = F.logsigmoid(y_pred).exp()\n",
    "\n",
    "        bs = y_true.size(0)\n",
    "        num_classes = y_pred.size(1)\n",
    "        dims = (0, 2)\n",
    "\n",
    "        if self.mode == BINARY_MODE:\n",
    "            y_true = y_true.view(bs, 1, -1)\n",
    "            y_pred = y_pred.view(bs, 1, -1)\n",
    "\n",
    "            if self.ignore_index is not None:\n",
    "                mask = y_true != self.ignore_index\n",
    "                y_pred = y_pred * mask\n",
    "                y_true = y_true * mask\n",
    "\n",
    "        if self.mode == MULTICLASS_MODE:\n",
    "            y_true = y_true.view(bs, -1)\n",
    "            y_pred = y_pred.view(bs, num_classes, -1)\n",
    "\n",
    "            if self.ignore_index is not None:\n",
    "                mask = y_true != self.ignore_index\n",
    "                y_pred = y_pred * mask.unsqueeze(1)\n",
    "\n",
    "                y_true = F.one_hot((y_true * mask).to(torch.long), num_classes)  # N,H*W -> N,H*W, C\n",
    "                y_true = y_true.permute(0, 2, 1) * mask.unsqueeze(1)  # H, C, H*W\n",
    "            else:\n",
    "                y_true = F.one_hot(y_true, num_classes)  # N,H*W -> N,H*W, C\n",
    "                y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n",
    "\n",
    "        if self.mode == MULTILABEL_MODE:\n",
    "            y_true = y_true.view(bs, num_classes, -1)\n",
    "            y_pred = y_pred.view(bs, num_classes, -1)\n",
    "\n",
    "            if self.ignore_index is not None:\n",
    "                mask = y_true != self.ignore_index\n",
    "                y_pred = y_pred * mask\n",
    "                y_true = y_true * mask\n",
    "\n",
    "        scores = soft_dice_score(y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims)\n",
    "\n",
    "        if self.log_loss:\n",
    "            loss = -torch.log(scores.clamp_min(self.eps))\n",
    "        else:\n",
    "            loss = 1.0 - scores\n",
    "\n",
    "        # Dice loss is undefined for non-empty classes\n",
    "        # So we zero contribution of channel that does not have true pixels\n",
    "        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n",
    "        # for this case, however it will be a modified jaccard loss\n",
    "\n",
    "        mask = y_true.sum(dims) > 0\n",
    "        loss *= mask.to(loss.dtype)\n",
    "\n",
    "        if self.classes is not None:\n",
    "            loss = loss[self.classes]\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d45b7f9d-c827-4ad7-9c53-c7f28eaf5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_loss(nn.Module):\n",
    "    def __init__(self, reduction='mean', lamb=1.25):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.lamb = lamb\n",
    "        self.base_loss = torch.nn.CrossEntropyLoss(reduction=reduction)\n",
    "        self.dice_loss = DiceLoss(mode='multiclass')\n",
    "        \n",
    "    def forward(self, logits, target):\n",
    "        base_l = self.base_loss(logits, target)\n",
    "        dice_l = self.dice_loss(logits, target)\n",
    "        \n",
    "        return base_l + self.lamb * dice_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b29467bb-4462-40a2-9450-54e74a65d3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yifen\\AppData\\Local\\Temp\\ipykernel_6336\\3521336878.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 50\n",
      "Epoch [51/250], Training Loss: 0.0954\n",
      "Epoch [51/250], Validation Loss: 0.1741, Validation mIoU: 0.8246\n",
      "Epoch [52/250], Training Loss: 0.0946\n",
      "Epoch [52/250], Validation Loss: 0.1644, Validation mIoU: 0.8314\n",
      "Epoch [53/250], Training Loss: 0.0922\n",
      "Epoch [53/250], Validation Loss: 0.1726, Validation mIoU: 0.8222\n",
      "Epoch [54/250], Training Loss: 0.0911\n",
      "Epoch [54/250], Validation Loss: 0.1526, Validation mIoU: 0.8379\n",
      "Epoch [55/250], Training Loss: 0.0945\n",
      "Epoch [55/250], Validation Loss: 0.1835, Validation mIoU: 0.8172\n",
      "Epoch [56/250], Training Loss: 0.0952\n",
      "Epoch [56/250], Validation Loss: 0.1657, Validation mIoU: 0.8320\n",
      "Epoch [57/250], Training Loss: 0.0884\n",
      "Epoch [57/250], Validation Loss: 0.1588, Validation mIoU: 0.8379\n",
      "Epoch [58/250], Training Loss: 0.0892\n",
      "Epoch [58/250], Validation Loss: 0.1604, Validation mIoU: 0.8366\n",
      "Epoch [59/250], Training Loss: 0.1059\n",
      "Epoch [59/250], Validation Loss: 0.1783, Validation mIoU: 0.8213\n",
      "Epoch [60/250], Training Loss: 0.0976\n",
      "Epoch [60/250], Validation Loss: 0.1658, Validation mIoU: 0.8324\n",
      "Epoch [61/250], Training Loss: 0.0956\n",
      "Epoch [61/250], Validation Loss: 0.1619, Validation mIoU: 0.8303\n",
      "Epoch [62/250], Training Loss: 0.0873\n",
      "Epoch [62/250], Validation Loss: 0.1707, Validation mIoU: 0.8283\n",
      "Epoch [63/250], Training Loss: 0.0897\n",
      "Epoch [63/250], Validation Loss: 0.1660, Validation mIoU: 0.8336\n",
      "Epoch [64/250], Training Loss: 0.0890\n",
      "Epoch [64/250], Validation Loss: 0.1593, Validation mIoU: 0.8342\n",
      "Epoch [65/250], Training Loss: 0.0873\n",
      "Epoch [65/250], Validation Loss: 0.1587, Validation mIoU: 0.8391\n",
      "Epoch [66/250], Training Loss: 0.0811\n",
      "Epoch [66/250], Validation Loss: 0.1575, Validation mIoU: 0.8413\n",
      "Epoch [67/250], Training Loss: 0.0795\n",
      "Epoch [67/250], Validation Loss: 0.1603, Validation mIoU: 0.8366\n",
      "Epoch [68/250], Training Loss: 0.0820\n",
      "Epoch [68/250], Validation Loss: 0.1590, Validation mIoU: 0.8398\n",
      "Epoch [69/250], Training Loss: 0.0822\n",
      "Epoch [69/250], Validation Loss: 0.1608, Validation mIoU: 0.8358\n",
      "Epoch [70/250], Training Loss: 0.0875\n",
      "Epoch [70/250], Validation Loss: 0.1667, Validation mIoU: 0.8304\n",
      "Epoch [71/250], Training Loss: 0.0819\n",
      "Epoch [71/250], Validation Loss: 0.1556, Validation mIoU: 0.8412\n",
      "Epoch [72/250], Training Loss: 0.0797\n",
      "Epoch [72/250], Validation Loss: 0.1574, Validation mIoU: 0.8413\n",
      "Epoch [73/250], Training Loss: 0.0778\n",
      "Epoch [73/250], Validation Loss: 0.1549, Validation mIoU: 0.8352\n",
      "Epoch [74/250], Training Loss: 0.0776\n",
      "Epoch [74/250], Validation Loss: 0.1566, Validation mIoU: 0.8436\n",
      "Epoch [75/250], Training Loss: 0.0809\n",
      "Epoch [75/250], Validation Loss: 0.1632, Validation mIoU: 0.8326\n",
      "Epoch [76/250], Training Loss: 0.0766\n",
      "Epoch [76/250], Validation Loss: 0.1556, Validation mIoU: 0.8424\n",
      "Epoch [77/250], Training Loss: 0.0735\n",
      "Epoch [77/250], Validation Loss: 0.1562, Validation mIoU: 0.8412\n",
      "Epoch [78/250], Training Loss: 0.0760\n",
      "Epoch [78/250], Validation Loss: 0.1490, Validation mIoU: 0.8486\n",
      "Epoch [79/250], Training Loss: 0.0795\n",
      "Epoch [79/250], Validation Loss: 0.1643, Validation mIoU: 0.8279\n",
      "Epoch [80/250], Training Loss: 0.0788\n",
      "Epoch [80/250], Validation Loss: 0.1579, Validation mIoU: 0.8398\n",
      "Epoch [81/250], Training Loss: 0.0757\n",
      "Epoch [81/250], Validation Loss: 0.1581, Validation mIoU: 0.8375\n",
      "Epoch [82/250], Training Loss: 0.0716\n",
      "Epoch [82/250], Validation Loss: 0.1601, Validation mIoU: 0.8361\n",
      "Epoch [83/250], Training Loss: 0.0692\n",
      "Epoch [83/250], Validation Loss: 0.1561, Validation mIoU: 0.8405\n",
      "Epoch [84/250], Training Loss: 0.0675\n",
      "Epoch [84/250], Validation Loss: 0.1451, Validation mIoU: 0.8493\n",
      "Epoch [85/250], Training Loss: 0.0706\n",
      "Epoch [85/250], Validation Loss: 0.1518, Validation mIoU: 0.8447\n",
      "Epoch [86/250], Training Loss: 0.0723\n",
      "Epoch [86/250], Validation Loss: 0.1534, Validation mIoU: 0.8442\n",
      "Epoch [87/250], Training Loss: 0.0684\n",
      "Epoch [87/250], Validation Loss: 0.1480, Validation mIoU: 0.8479\n",
      "Epoch [88/250], Training Loss: 0.0641\n",
      "Epoch [88/250], Validation Loss: 0.1437, Validation mIoU: 0.8519\n",
      "Epoch [89/250], Training Loss: 0.0792\n",
      "Epoch [89/250], Validation Loss: 0.1705, Validation mIoU: 0.8229\n",
      "Epoch [90/250], Training Loss: 0.0814\n",
      "Epoch [90/250], Validation Loss: 0.1574, Validation mIoU: 0.8425\n",
      "Epoch [91/250], Training Loss: 0.0712\n",
      "Epoch [91/250], Validation Loss: 0.1429, Validation mIoU: 0.8510\n",
      "Epoch [92/250], Training Loss: 0.0694\n",
      "Epoch [92/250], Validation Loss: 0.1534, Validation mIoU: 0.8417\n",
      "Epoch [93/250], Training Loss: 0.0712\n",
      "Epoch [93/250], Validation Loss: 0.1497, Validation mIoU: 0.8444\n",
      "Epoch [94/250], Training Loss: 0.0672\n",
      "Epoch [94/250], Validation Loss: 0.1477, Validation mIoU: 0.8473\n",
      "Epoch [95/250], Training Loss: 0.0665\n",
      "Epoch [95/250], Validation Loss: 0.1514, Validation mIoU: 0.8441\n",
      "Epoch [96/250], Training Loss: 0.0682\n",
      "Epoch [96/250], Validation Loss: 0.1455, Validation mIoU: 0.8494\n",
      "Epoch [97/250], Training Loss: 0.0630\n",
      "Epoch [97/250], Validation Loss: 0.1497, Validation mIoU: 0.8483\n",
      "Epoch [98/250], Training Loss: 0.0666\n",
      "Epoch [98/250], Validation Loss: 0.1567, Validation mIoU: 0.8356\n",
      "Epoch [99/250], Training Loss: 0.0718\n",
      "Epoch [99/250], Validation Loss: 0.1602, Validation mIoU: 0.8304\n",
      "Epoch [100/250], Training Loss: 0.0702\n",
      "Epoch [100/250], Validation Loss: 0.1431, Validation mIoU: 0.8531\n",
      "Epoch [101/250], Training Loss: 0.0640\n",
      "Epoch [101/250], Validation Loss: 0.1436, Validation mIoU: 0.8531\n",
      "Epoch [102/250], Training Loss: 0.0638\n",
      "Epoch [102/250], Validation Loss: 0.1435, Validation mIoU: 0.8524\n",
      "Epoch [103/250], Training Loss: 0.0617\n",
      "Epoch [103/250], Validation Loss: 0.1462, Validation mIoU: 0.8482\n",
      "Epoch [104/250], Training Loss: 0.0599\n",
      "Epoch [104/250], Validation Loss: 0.1459, Validation mIoU: 0.8493\n",
      "Epoch [105/250], Training Loss: 0.0607\n",
      "Epoch [105/250], Validation Loss: 0.1456, Validation mIoU: 0.8514\n",
      "Epoch [106/250], Training Loss: 0.0595\n",
      "Epoch [106/250], Validation Loss: 0.1482, Validation mIoU: 0.8486\n",
      "Epoch [107/250], Training Loss: 0.0631\n",
      "Epoch [107/250], Validation Loss: 0.1464, Validation mIoU: 0.8501\n",
      "Epoch [108/250], Training Loss: 0.0585\n",
      "Epoch [108/250], Validation Loss: 0.1501, Validation mIoU: 0.8456\n",
      "Epoch [109/250], Training Loss: 0.0577\n",
      "Epoch [109/250], Validation Loss: 0.1484, Validation mIoU: 0.8481\n",
      "Epoch [110/250], Training Loss: 0.0603\n",
      "Epoch [110/250], Validation Loss: 0.1489, Validation mIoU: 0.8467\n",
      "Epoch [111/250], Training Loss: 0.0588\n",
      "Epoch [111/250], Validation Loss: 0.1434, Validation mIoU: 0.8456\n",
      "Epoch [112/250], Training Loss: 0.0605\n",
      "Epoch [112/250], Validation Loss: 0.1447, Validation mIoU: 0.8501\n",
      "Epoch [113/250], Training Loss: 0.0571\n",
      "Epoch [113/250], Validation Loss: 0.1426, Validation mIoU: 0.8541\n",
      "Epoch [114/250], Training Loss: 0.0602\n",
      "Epoch [114/250], Validation Loss: 0.1706, Validation mIoU: 0.8310\n",
      "Epoch [115/250], Training Loss: 0.0667\n",
      "Epoch [115/250], Validation Loss: 0.1421, Validation mIoU: 0.8538\n",
      "Epoch [116/250], Training Loss: 0.0573\n",
      "Epoch [116/250], Validation Loss: 0.1364, Validation mIoU: 0.8521\n",
      "Epoch [117/250], Training Loss: 0.0596\n",
      "Epoch [117/250], Validation Loss: 0.1495, Validation mIoU: 0.8499\n",
      "Epoch [118/250], Training Loss: 0.0572\n",
      "Epoch [118/250], Validation Loss: 0.1447, Validation mIoU: 0.8506\n",
      "Epoch [119/250], Training Loss: 0.0557\n",
      "Epoch [119/250], Validation Loss: 0.1456, Validation mIoU: 0.8508\n",
      "Epoch [120/250], Training Loss: 0.0561\n",
      "Epoch [120/250], Validation Loss: 0.1441, Validation mIoU: 0.8527\n",
      "Epoch [121/250], Training Loss: 0.0541\n",
      "Epoch [121/250], Validation Loss: 0.1430, Validation mIoU: 0.8529\n",
      "Epoch [122/250], Training Loss: 0.0537\n",
      "Epoch [122/250], Validation Loss: 0.1435, Validation mIoU: 0.8541\n",
      "Epoch [123/250], Training Loss: 0.0522\n",
      "Epoch [123/250], Validation Loss: 0.1476, Validation mIoU: 0.8510\n",
      "Epoch [124/250], Training Loss: 0.0523\n",
      "Epoch [124/250], Validation Loss: 0.1446, Validation mIoU: 0.8511\n",
      "Epoch [125/250], Training Loss: 0.0536\n",
      "Epoch [125/250], Validation Loss: 0.1389, Validation mIoU: 0.8571\n",
      "Epoch [126/250], Training Loss: 0.0524\n",
      "Epoch [126/250], Validation Loss: 0.1515, Validation mIoU: 0.8479\n",
      "Epoch [127/250], Training Loss: 0.0541\n",
      "Epoch [127/250], Validation Loss: 0.1444, Validation mIoU: 0.8520\n",
      "Epoch [128/250], Training Loss: 0.0529\n",
      "Epoch [128/250], Validation Loss: 0.1474, Validation mIoU: 0.8493\n",
      "Epoch [129/250], Training Loss: 0.0532\n",
      "Epoch [129/250], Validation Loss: 0.1462, Validation mIoU: 0.8528\n",
      "Epoch [130/250], Training Loss: 0.0580\n",
      "Epoch [130/250], Validation Loss: 0.1470, Validation mIoU: 0.8498\n",
      "Epoch [131/250], Training Loss: 0.0751\n",
      "Epoch [131/250], Validation Loss: 0.1579, Validation mIoU: 0.8394\n",
      "Epoch [132/250], Training Loss: 0.0663\n",
      "Epoch [132/250], Validation Loss: 0.2197, Validation mIoU: 0.7882\n",
      "Epoch [133/250], Training Loss: 0.0642\n",
      "Epoch [133/250], Validation Loss: 0.1449, Validation mIoU: 0.8459\n",
      "Epoch [134/250], Training Loss: 0.0611\n",
      "Epoch [134/250], Validation Loss: 0.1439, Validation mIoU: 0.8528\n",
      "Epoch [135/250], Training Loss: 0.0553\n",
      "Epoch [135/250], Validation Loss: 0.1467, Validation mIoU: 0.8503\n",
      "Epoch [136/250], Training Loss: 0.0523\n",
      "Epoch [136/250], Validation Loss: 0.1396, Validation mIoU: 0.8568\n",
      "Epoch [137/250], Training Loss: 0.0526\n",
      "Epoch [137/250], Validation Loss: 0.1370, Validation mIoU: 0.8583\n",
      "Epoch [138/250], Training Loss: 0.0512\n",
      "Epoch [138/250], Validation Loss: 0.1394, Validation mIoU: 0.8584\n",
      "Epoch [139/250], Training Loss: 0.0498\n",
      "Epoch [139/250], Validation Loss: 0.1447, Validation mIoU: 0.8514\n",
      "Epoch [140/250], Training Loss: 0.0514\n",
      "Epoch [140/250], Validation Loss: 0.1439, Validation mIoU: 0.8548\n",
      "Epoch [141/250], Training Loss: 0.0524\n",
      "Epoch [141/250], Validation Loss: 0.1384, Validation mIoU: 0.8591\n",
      "Epoch [142/250], Training Loss: 0.0517\n",
      "Epoch [142/250], Validation Loss: 0.1411, Validation mIoU: 0.8553\n",
      "Epoch [143/250], Training Loss: 0.0532\n",
      "Epoch [143/250], Validation Loss: 0.1474, Validation mIoU: 0.8494\n",
      "Epoch [144/250], Training Loss: 0.0503\n",
      "Epoch [144/250], Validation Loss: 0.1405, Validation mIoU: 0.8559\n",
      "Epoch [145/250], Training Loss: 0.0493\n",
      "Epoch [145/250], Validation Loss: 0.1386, Validation mIoU: 0.8584\n",
      "Epoch [146/250], Training Loss: 0.0487\n",
      "Epoch [146/250], Validation Loss: 0.1443, Validation mIoU: 0.8553\n",
      "Epoch [147/250], Training Loss: 0.0479\n",
      "Epoch [147/250], Validation Loss: 0.1469, Validation mIoU: 0.8525\n",
      "Epoch [148/250], Training Loss: 0.0482\n",
      "Epoch [148/250], Validation Loss: 0.1487, Validation mIoU: 0.8510\n",
      "Epoch [149/250], Training Loss: 0.0466\n",
      "Epoch [149/250], Validation Loss: 0.1415, Validation mIoU: 0.8566\n",
      "Epoch [150/250], Training Loss: 0.0480\n",
      "Epoch [150/250], Validation Loss: 0.1478, Validation mIoU: 0.8502\n",
      "Epoch [151/250], Training Loss: 0.0465\n",
      "Epoch [151/250], Validation Loss: 0.1402, Validation mIoU: 0.8577\n",
      "Epoch [152/250], Training Loss: 0.0492\n",
      "Epoch [152/250], Validation Loss: 0.1445, Validation mIoU: 0.8539\n",
      "Epoch [153/250], Training Loss: 0.0494\n",
      "Epoch [153/250], Validation Loss: 0.1437, Validation mIoU: 0.8557\n",
      "Epoch [154/250], Training Loss: 0.0501\n",
      "Epoch [154/250], Validation Loss: 0.1491, Validation mIoU: 0.8484\n",
      "Epoch [155/250], Training Loss: 0.0490\n",
      "Epoch [155/250], Validation Loss: 0.1467, Validation mIoU: 0.8529\n",
      "Epoch [156/250], Training Loss: 0.0474\n",
      "Epoch [156/250], Validation Loss: 0.1423, Validation mIoU: 0.8548\n",
      "Epoch [157/250], Training Loss: 0.0522\n",
      "Epoch [157/250], Validation Loss: 0.1447, Validation mIoU: 0.8509\n",
      "Epoch [158/250], Training Loss: 0.0469\n",
      "Epoch [158/250], Validation Loss: 0.1414, Validation mIoU: 0.8567\n",
      "Epoch [159/250], Training Loss: 0.0460\n",
      "Epoch [159/250], Validation Loss: 0.1471, Validation mIoU: 0.8507\n",
      "Epoch [160/250], Training Loss: 0.0455\n",
      "Epoch [160/250], Validation Loss: 0.1408, Validation mIoU: 0.8570\n",
      "Epoch [161/250], Training Loss: 0.0458\n",
      "Epoch [161/250], Validation Loss: 0.1429, Validation mIoU: 0.8545\n",
      "Epoch [162/250], Training Loss: 0.0459\n",
      "Epoch [162/250], Validation Loss: 0.1529, Validation mIoU: 0.8498\n",
      "Epoch [163/250], Training Loss: 0.0506\n",
      "Epoch [163/250], Validation Loss: 0.1435, Validation mIoU: 0.8536\n",
      "Epoch [164/250], Training Loss: 0.0469\n",
      "Epoch [164/250], Validation Loss: 0.1390, Validation mIoU: 0.8577\n",
      "Epoch [165/250], Training Loss: 0.0456\n",
      "Epoch [165/250], Validation Loss: 0.1371, Validation mIoU: 0.8596\n",
      "Epoch [166/250], Training Loss: 0.0443\n",
      "Epoch [166/250], Validation Loss: 0.1432, Validation mIoU: 0.8552\n",
      "Epoch [167/250], Training Loss: 0.0465\n",
      "Epoch [167/250], Validation Loss: 0.1492, Validation mIoU: 0.8485\n",
      "Epoch [168/250], Training Loss: 0.0455\n",
      "Epoch [168/250], Validation Loss: 0.1427, Validation mIoU: 0.8560\n",
      "Epoch [169/250], Training Loss: 0.0462\n",
      "Epoch [169/250], Validation Loss: 0.1464, Validation mIoU: 0.8514\n",
      "Epoch [170/250], Training Loss: 0.0430\n",
      "Epoch [170/250], Validation Loss: 0.1408, Validation mIoU: 0.8579\n",
      "Epoch [171/250], Training Loss: 0.0443\n",
      "Epoch [171/250], Validation Loss: 0.1408, Validation mIoU: 0.8577\n",
      "Epoch [172/250], Training Loss: 0.0441\n",
      "Epoch [172/250], Validation Loss: 0.1471, Validation mIoU: 0.8526\n",
      "Epoch [173/250], Training Loss: 0.0428\n",
      "Epoch [173/250], Validation Loss: 0.1434, Validation mIoU: 0.8561\n",
      "Epoch [174/250], Training Loss: 0.0423\n",
      "Epoch [174/250], Validation Loss: 0.1426, Validation mIoU: 0.8556\n",
      "Epoch [175/250], Training Loss: 0.0439\n",
      "Epoch [175/250], Validation Loss: 0.1404, Validation mIoU: 0.8513\n",
      "Epoch [176/250], Training Loss: 0.0445\n",
      "Epoch [176/250], Validation Loss: 0.1489, Validation mIoU: 0.8514\n",
      "Epoch [177/250], Training Loss: 0.0443\n",
      "Epoch [177/250], Validation Loss: 0.1423, Validation mIoU: 0.8567\n",
      "Epoch [178/250], Training Loss: 0.0440\n",
      "Epoch [178/250], Validation Loss: 0.1484, Validation mIoU: 0.8523\n",
      "Epoch [179/250], Training Loss: 0.0415\n",
      "Epoch [179/250], Validation Loss: 0.1416, Validation mIoU: 0.8516\n",
      "Epoch [180/250], Training Loss: 0.0418\n",
      "Epoch [180/250], Validation Loss: 0.1502, Validation mIoU: 0.8498\n",
      "Epoch [181/250], Training Loss: 0.0456\n",
      "Epoch [181/250], Validation Loss: 0.1463, Validation mIoU: 0.8527\n",
      "Epoch [182/250], Training Loss: 0.0466\n",
      "Epoch [182/250], Validation Loss: 0.1499, Validation mIoU: 0.8499\n",
      "Epoch [183/250], Training Loss: 0.0435\n",
      "Epoch [183/250], Validation Loss: 0.1462, Validation mIoU: 0.8557\n",
      "Epoch [184/250], Training Loss: 0.0431\n",
      "Epoch [184/250], Validation Loss: 0.1450, Validation mIoU: 0.8550\n",
      "Epoch [185/250], Training Loss: 0.0397\n",
      "Epoch [185/250], Validation Loss: 0.1444, Validation mIoU: 0.8544\n",
      "Epoch [186/250], Training Loss: 0.0429\n",
      "Epoch [186/250], Validation Loss: 0.1447, Validation mIoU: 0.8544\n",
      "Epoch [187/250], Training Loss: 0.0424\n",
      "Epoch [187/250], Validation Loss: 0.1448, Validation mIoU: 0.8552\n",
      "Epoch [188/250], Training Loss: 0.0396\n",
      "Epoch [188/250], Validation Loss: 0.1463, Validation mIoU: 0.8533\n",
      "Epoch [189/250], Training Loss: 0.0391\n",
      "Epoch [189/250], Validation Loss: 0.1462, Validation mIoU: 0.8539\n",
      "Epoch [190/250], Training Loss: 0.0396\n",
      "Epoch [190/250], Validation Loss: 0.1461, Validation mIoU: 0.8538\n",
      "Epoch [191/250], Training Loss: 0.0394\n",
      "Epoch [191/250], Validation Loss: 0.1439, Validation mIoU: 0.8551\n",
      "Epoch [192/250], Training Loss: 0.0398\n",
      "Epoch [192/250], Validation Loss: 0.1450, Validation mIoU: 0.8546\n",
      "Epoch [193/250], Training Loss: 0.0410\n",
      "Epoch [193/250], Validation Loss: 0.1466, Validation mIoU: 0.8471\n",
      "Epoch [194/250], Training Loss: 0.0416\n",
      "Epoch [194/250], Validation Loss: 0.1502, Validation mIoU: 0.8440\n",
      "Epoch [195/250], Training Loss: 0.0477\n",
      "Epoch [195/250], Validation Loss: 0.1552, Validation mIoU: 0.8464\n",
      "Epoch [196/250], Training Loss: 0.0447\n",
      "Epoch [196/250], Validation Loss: 0.1528, Validation mIoU: 0.8485\n",
      "Epoch [197/250], Training Loss: 0.0412\n",
      "Epoch [197/250], Validation Loss: 0.1461, Validation mIoU: 0.8555\n",
      "Epoch [198/250], Training Loss: 0.0389\n",
      "Epoch [198/250], Validation Loss: 0.1481, Validation mIoU: 0.8531\n",
      "Epoch [199/250], Training Loss: 0.0382\n",
      "Epoch [199/250], Validation Loss: 0.1492, Validation mIoU: 0.8460\n",
      "Epoch [200/250], Training Loss: 0.0385\n",
      "Epoch [200/250], Validation Loss: 0.1454, Validation mIoU: 0.8564\n",
      "Epoch [201/250], Training Loss: 0.0390\n",
      "Epoch [201/250], Validation Loss: 0.1515, Validation mIoU: 0.8502\n",
      "Epoch [202/250], Training Loss: 0.0392\n",
      "Epoch [202/250], Validation Loss: 0.1478, Validation mIoU: 0.8523\n",
      "Epoch [203/250], Training Loss: 0.0379\n",
      "Epoch [203/250], Validation Loss: 0.1473, Validation mIoU: 0.8542\n",
      "Epoch [204/250], Training Loss: 0.0376\n",
      "Epoch [204/250], Validation Loss: 0.1501, Validation mIoU: 0.8516\n",
      "Epoch [205/250], Training Loss: 0.0482\n",
      "Epoch [205/250], Validation Loss: 0.1565, Validation mIoU: 0.8460\n",
      "Epoch [206/250], Training Loss: 0.0458\n",
      "Epoch [206/250], Validation Loss: 0.1484, Validation mIoU: 0.8454\n",
      "Epoch [207/250], Training Loss: 0.0419\n",
      "Epoch [207/250], Validation Loss: 0.1492, Validation mIoU: 0.8524\n",
      "Epoch [208/250], Training Loss: 0.0397\n",
      "Epoch [208/250], Validation Loss: 0.1503, Validation mIoU: 0.8511\n",
      "Epoch [209/250], Training Loss: 0.0395\n",
      "Epoch [209/250], Validation Loss: 0.1444, Validation mIoU: 0.8558\n",
      "Epoch [210/250], Training Loss: 0.0394\n",
      "Epoch [210/250], Validation Loss: 0.1528, Validation mIoU: 0.8492\n",
      "Epoch [211/250], Training Loss: 0.0364\n",
      "Epoch [211/250], Validation Loss: 0.1476, Validation mIoU: 0.8533\n",
      "Epoch [212/250], Training Loss: 0.0370\n",
      "Epoch [212/250], Validation Loss: 0.1465, Validation mIoU: 0.8538\n",
      "Epoch [213/250], Training Loss: 0.0356\n",
      "Epoch [213/250], Validation Loss: 0.1501, Validation mIoU: 0.8519\n",
      "Epoch [214/250], Training Loss: 0.0353\n",
      "Epoch [214/250], Validation Loss: 0.1504, Validation mIoU: 0.8526\n",
      "Epoch [215/250], Training Loss: 0.0380\n",
      "Epoch [215/250], Validation Loss: 0.1519, Validation mIoU: 0.8505\n",
      "Epoch [216/250], Training Loss: 0.0403\n",
      "Epoch [216/250], Validation Loss: 0.1449, Validation mIoU: 0.8502\n",
      "Epoch [217/250], Training Loss: 0.0368\n",
      "Epoch [217/250], Validation Loss: 0.1456, Validation mIoU: 0.8559\n",
      "Epoch [218/250], Training Loss: 0.0385\n",
      "Epoch [218/250], Validation Loss: 0.1502, Validation mIoU: 0.8511\n",
      "Epoch [219/250], Training Loss: 0.0389\n",
      "Epoch [219/250], Validation Loss: 0.1496, Validation mIoU: 0.8447\n",
      "Epoch [220/250], Training Loss: 0.0372\n",
      "Epoch [220/250], Validation Loss: 0.1502, Validation mIoU: 0.8522\n",
      "Epoch [221/250], Training Loss: 0.0360\n",
      "Epoch [221/250], Validation Loss: 0.1491, Validation mIoU: 0.8457\n",
      "Epoch [222/250], Training Loss: 0.0365\n",
      "Epoch [222/250], Validation Loss: 0.1495, Validation mIoU: 0.8515\n",
      "Epoch [223/250], Training Loss: 0.0354\n",
      "Epoch [223/250], Validation Loss: 0.1449, Validation mIoU: 0.8558\n",
      "Epoch [224/250], Training Loss: 0.0346\n",
      "Epoch [224/250], Validation Loss: 0.1493, Validation mIoU: 0.8465\n",
      "Epoch [225/250], Training Loss: 0.0376\n",
      "Epoch [225/250], Validation Loss: 0.1447, Validation mIoU: 0.8503\n",
      "Epoch [226/250], Training Loss: 0.0355\n",
      "Epoch [226/250], Validation Loss: 0.1531, Validation mIoU: 0.8498\n",
      "Epoch [227/250], Training Loss: 0.0358\n",
      "Epoch [227/250], Validation Loss: 0.1626, Validation mIoU: 0.8402\n",
      "Epoch [228/250], Training Loss: 0.0408\n",
      "Epoch [228/250], Validation Loss: 0.1458, Validation mIoU: 0.8477\n",
      "Epoch [229/250], Training Loss: 0.0445\n",
      "Epoch [229/250], Validation Loss: 0.1442, Validation mIoU: 0.8564\n",
      "Epoch [230/250], Training Loss: 0.0400\n",
      "Epoch [230/250], Validation Loss: 0.1484, Validation mIoU: 0.8536\n",
      "Epoch [231/250], Training Loss: 0.0370\n",
      "Epoch [231/250], Validation Loss: 0.1493, Validation mIoU: 0.8528\n",
      "Epoch [232/250], Training Loss: 0.0378\n",
      "Epoch [232/250], Validation Loss: 0.1523, Validation mIoU: 0.8505\n",
      "Epoch [233/250], Training Loss: 0.0360\n",
      "Epoch [233/250], Validation Loss: 0.1483, Validation mIoU: 0.8535\n",
      "Epoch [234/250], Training Loss: 0.0336\n",
      "Epoch [234/250], Validation Loss: 0.1478, Validation mIoU: 0.8547\n",
      "Epoch [235/250], Training Loss: 0.0348\n",
      "Epoch [235/250], Validation Loss: 0.1449, Validation mIoU: 0.8578\n",
      "Epoch [236/250], Training Loss: 0.0350\n",
      "Epoch [236/250], Validation Loss: 0.1449, Validation mIoU: 0.8572\n",
      "Epoch [237/250], Training Loss: 0.0348\n",
      "Epoch [237/250], Validation Loss: 0.1506, Validation mIoU: 0.8529\n",
      "Epoch [238/250], Training Loss: 0.0359\n",
      "Epoch [238/250], Validation Loss: 0.1581, Validation mIoU: 0.8466\n",
      "Epoch [239/250], Training Loss: 0.0409\n",
      "Epoch [239/250], Validation Loss: 0.1516, Validation mIoU: 0.8515\n",
      "Epoch [240/250], Training Loss: 0.0408\n",
      "Epoch [240/250], Validation Loss: 0.1418, Validation mIoU: 0.8593\n",
      "Epoch [241/250], Training Loss: 0.0352\n",
      "Epoch [241/250], Validation Loss: 0.1445, Validation mIoU: 0.8579\n",
      "Epoch [242/250], Training Loss: 0.0331\n",
      "Epoch [242/250], Validation Loss: 0.1426, Validation mIoU: 0.8595\n",
      "Epoch [243/250], Training Loss: 0.0337\n",
      "Epoch [243/250], Validation Loss: 0.1490, Validation mIoU: 0.8479\n",
      "Epoch [244/250], Training Loss: 0.0434\n",
      "Epoch [244/250], Validation Loss: 0.1533, Validation mIoU: 0.8480\n",
      "Epoch [245/250], Training Loss: 0.0367\n",
      "Epoch [245/250], Validation Loss: 0.1479, Validation mIoU: 0.8534\n",
      "Epoch [246/250], Training Loss: 0.0336\n",
      "Epoch [246/250], Validation Loss: 0.1488, Validation mIoU: 0.8530\n",
      "Epoch [247/250], Training Loss: 0.0396\n",
      "Epoch [247/250], Validation Loss: 0.1490, Validation mIoU: 0.8524\n",
      "Epoch [248/250], Training Loss: 0.0374\n",
      "Epoch [248/250], Validation Loss: 0.1496, Validation mIoU: 0.8520\n",
      "Epoch [249/250], Training Loss: 0.0348\n",
      "Epoch [249/250], Validation Loss: 0.1460, Validation mIoU: 0.8552\n",
      "Epoch [250/250], Training Loss: 0.0330\n",
      "Epoch [250/250], Validation Loss: 0.1446, Validation mIoU: 0.8581\n",
      "Finished Training\n",
      "Model weights saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model\n",
    "model = segformer\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function (using CrossEntropyLoss for multi-class segmentation)\n",
    "criterion = weighted_loss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = 'Segformer_Scratch(V1)_checkpoint.pth'\n",
    "start_epoch = 0  # Initialize start epoch\n",
    "\n",
    "try:\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Checkpoint not found. Starting from scratch.\")\n",
    "\n",
    "num_epochs = 250  # Total epochs to train\n",
    "\n",
    "# Continue training from the loaded checkpoint\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        images = batch['pixel_values'].to(device)\n",
    "        masks = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        logits = outputs  # Directly use the output as logits\n",
    "\n",
    "        # Resize logits to match the mask size\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits, size=masks.shape[-2:], mode='bilinear', align_corners=False\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(upsampled_logits, masks)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    iou_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            masks = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            logits = outputs  # Directly use the output as logits\n",
    "\n",
    "            # Resize logits to match mask size\n",
    "            upsampled_logits = F.interpolate(\n",
    "                logits, size=masks.shape[-2:], mode='bilinear', align_corners=False\n",
    "            )\n",
    "\n",
    "            loss = criterion(upsampled_logits, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute predictions\n",
    "            _, preds = torch.max(upsampled_logits, 1)\n",
    "            ious = compute_iou(preds, masks, num_classes=9)\n",
    "            iou_list.append(ious)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    iou_list = np.array(iou_list)\n",
    "    mean_iou_per_class = np.nanmean(iou_list, axis=0)\n",
    "    mIoU = np.nanmean(mean_iou_per_class)\n",
    "\n",
    "    # Save model and optimizer states\n",
    "    torch.save({\n",
    "        'epoch': epoch,  # Save the current epoch number\n",
    "        'model_state_dict': model.state_dict(),  # Save model parameters\n",
    "        'optimizer_state_dict': optimizer.state_dict(),  # Save optimizer parameters\n",
    "        'loss': loss,  # Optionally, save the loss value if needed\n",
    "        'train_loss': train_loss,  # Save training losses\n",
    "        'val_losses': val_loss,  # Save validation losses       \n",
    "    }, 'Segformer_Scratch(V2)_checkpoint.pth')  # File name where the checkpoint will be saved\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation mIoU: {mIoU:.4f}\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# After training is completed, save the model's weights\n",
    "torch.save(model.state_dict(),'Segformer_Scratch(V2)_weights.pth')\n",
    "print(\"Model weights saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaace144-65b7-4a13-8ffc-ac60b6944fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2893, Val mIoU: 0.7764\n",
      "Class 0 (background): IoU = 0.9886\n",
      "Class 1 (spleen): IoU = 0.9316\n",
      "Class 2 (right kidney): IoU = 0.8702\n",
      "Class 3 (left kidney): IoU = 0.9015\n",
      "Class 4 (gallbladder): IoU = 0.0000\n",
      "Class 5 (pancreas): IoU = 0.9178\n",
      "Class 6 (liver): IoU = 0.8685\n",
      "Class 7 (stomach): IoU = 0.8532\n",
      "Class 8 (aorta): IoU = 0.6564\n"
     ]
    }
   ],
   "source": [
    "num_classes = 9  # Including background\n",
    "\n",
    "class_names = ['background', 'spleen', 'right kidney', 'left kidney', 'gallbladder', 'pancreas', 'liver', 'stomach', 'aorta']\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "iou_list = []\n",
    "with torch.no_grad():\n",
    "    for images, masks in val_loader:\n",
    "        images = batch['pixel_values'].to(device)\n",
    "        masks = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        logits = outputs\n",
    "\n",
    "        # Resize logits to match mask size\n",
    "        upsampled_logits = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        loss = criterion(upsampled_logits, masks)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Compute predictions\n",
    "        _, preds = torch.max(upsampled_logits, 1)\n",
    "        ious = compute_iou(preds, masks, num_classes=9)\n",
    "        iou_list.append(ious)\n",
    "\n",
    "avg_test_loss = test_loss / len(val_loader)\n",
    "iou_list = np.array(iou_list)\n",
    "mean_iou_per_class = np.nanmean(iou_list, axis=0)\n",
    "mIoU = np.nanmean(mean_iou_per_class)\n",
    "\n",
    "print(f\"Val Loss: {avg_test_loss:.4f}, Val mIoU: {mIoU:.4f}\")\n",
    "\n",
    "# Display IoU for each class\n",
    "for idx, iou in enumerate(mean_iou_per_class):\n",
    "    print(f\"Class {idx} ({class_names[idx]}): IoU = {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d3724-a786-4d16-9d8d-02db9d23c922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
